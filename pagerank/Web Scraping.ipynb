{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import urllib\n",
    "import ssl \n",
    "from urlparse import urljoin\n",
    "from urlparse import urlparse\n",
    "from BeautifulSoup import *\n",
    "scontext = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('data.sqlite')\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x112ff2180>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.execute('''CREATE TABLE IF NOT EXISTS Pages \n",
    "    (id INTEGER PRIMARY KEY, url TEXT UNIQUE, html TEXT, \n",
    "     error INTEGER, old_rank REAL, new_rank REAL)''')\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Links \n",
    "    (from_id INTEGER, to_id INTEGER)''')\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Webs (url TEXT UNIQUE)''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restarting existing crawl.\n"
     ]
    }
   ],
   "source": [
    "cur.execute('SELECT id,url FROM Pages WHERE html is NULL and error is NULL ORDER BY RANDOM() LIMIT 1')\n",
    "row = cur.fetchone()\n",
    "if row is not None:\n",
    "    print \"Restarting existing crawl.\"\n",
    "else :\n",
    "    starturl = raw_input('Enter web url or enter: ')\n",
    "    if ( len(starturl) < 1 ) : starturl = 'http://python-data.dr-chuck.net/'\n",
    "    if ( starturl.endswith('/') ) : starturl = starturl[:-1]\n",
    "    web = starturl\n",
    "    if ( starturl.endswith('.htm') or starturl.endswith('.html') ) :\n",
    "        pos = starturl.rfind('/')\n",
    "        web = starturl[:pos]\n",
    "\n",
    "    if ( len(web) > 1 ) :\n",
    "        cur.execute('INSERT OR IGNORE INTO Webs (url) VALUES ( ? )', ( web, ) )\n",
    "        cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( starturl, ) ) \n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['www.columbia.edu', 'http://python-data.dr-chuck.net', 'www.coursera.org']\n"
     ]
    }
   ],
   "source": [
    "cur.execute('''SELECT url FROM Webs''')\n",
    "webs = list()\n",
    "for row in cur:\n",
    "    webs.append(str(row[0]))\n",
    "\n",
    "print webs\n",
    "\n",
    "num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many pages:50\n",
      "1267 http://python-data.dr-chuck.net/known_by_Karlie.html (12053) 100\n",
      "1709 http://python-data.dr-chuck.net/known_by_Kainui.html (11990) 100\n",
      "957 http://python-data.dr-chuck.net/known_by_Mahnoor.html (12050) 100\n",
      "2808 http://python-data.dr-chuck.net/known_by_Simonne.html (12083) 100\n",
      "2539 http://python-data.dr-chuck.net/known_by_Eshaal.html (12037) 100\n",
      "2496 http://python-data.dr-chuck.net/known_by_Sorcha.html (12008) 100\n",
      "349 http://python-data.dr-chuck.net/known_by_Jayse.html (12022) 100\n",
      "2776 http://python-data.dr-chuck.net/known_by_Aaran.html (12046) 100\n",
      "306 http://python-data.dr-chuck.net/known_by_Nazzera.html (12090) 100\n",
      "2023 http://python-data.dr-chuck.net/known_by_Devon.html (12079) 100\n",
      "2517 http://python-data.dr-chuck.net/known_by_Pearsen.html (12092) 100\n",
      "1794 http://python-data.dr-chuck.net/known_by_Carter.html (11998) 100\n",
      "1260 http://python-data.dr-chuck.net/known_by_Teejay.html (12099) 100\n",
      "2143 http://python-data.dr-chuck.net/known_by_Machlan.html (12043) 100\n",
      "1823 http://python-data.dr-chuck.net/known_by_Julita.html (12060) 100\n",
      "1290 http://python-data.dr-chuck.net/known_by_Ali.html (12035) 100\n",
      "2265 http://python-data.dr-chuck.net/known_by_Fenn.html (12108) 100\n",
      "1418 http://python-data.dr-chuck.net/known_by_Denny.html (11997) 100\n",
      "2696 http://python-data.dr-chuck.net/known_by_Elish.html (12025) 100\n",
      "2472 http://python-data.dr-chuck.net/known_by_Ailiegh.html (12029) 100\n",
      "2518 http://python-data.dr-chuck.net/known_by_Emil.html (12033) 100\n",
      "77 http://python-data.dr-chuck.net/known_by_Agatha.html (12031) 100\n",
      "2617 http://python-data.dr-chuck.net/known_by_Kurt.html (12040) 100\n",
      "2853 http://python-data.dr-chuck.net/known_by_Caoilainn.html (12039) 100\n",
      "3266 http://python-data.dr-chuck.net/known_by_Jaida.html (12070) 100\n",
      "3530 http://python-data.dr-chuck.net/known_by_Amos.html (12039) 100\n",
      "1037 http://python-data.dr-chuck.net/known_by_Safia.html (12066) 100\n",
      "951 http://python-data.dr-chuck.net/known_by_Sorche.html (12081) 100\n",
      "186 http://python-data.dr-chuck.net/known_by_Anabelle.html (12084) 100\n",
      "3830 http://python-data.dr-chuck.net/known_by_Anneroy.html (12032) 100\n",
      "3734 http://python-data.dr-chuck.net/known_by_Daegyu.html (11996) 100\n",
      "829 http://python-data.dr-chuck.net/known_by_Naoise.html (12044) 100\n",
      "2274 http://python-data.dr-chuck.net/known_by_Chester.html (12112) 100\n",
      "856 http://python-data.dr-chuck.net/known_by_Harper.html (12105) 100\n",
      "1767 http://python-data.dr-chuck.net/known_by_Christina.html (12051) 100\n",
      "3974 http://python-data.dr-chuck.net/known_by_Kye.html (12035) 100\n",
      "2914 http://python-data.dr-chuck.net/known_by_Chardonnay.html (12074) 100\n",
      "2404 http://python-data.dr-chuck.net/known_by_Rosheen.html (12126) 100\n",
      "2320 http://python-data.dr-chuck.net/known_by_Zaak.html (11994) 100\n",
      "190 http://python-data.dr-chuck.net/known_by_Jostelle.html (12014) 100\n",
      "2035 http://python-data.dr-chuck.net/known_by_Nitya.html (12046) 100\n",
      "2741 http://python-data.dr-chuck.net/known_by_Kornelija.html (12024) 100\n",
      "2433 http://python-data.dr-chuck.net/known_by_Nolan.html (12071) 100\n",
      "2029 http://python-data.dr-chuck.net/known_by_Alistair.html (12024) 100\n",
      "1568 http://python-data.dr-chuck.net/known_by_Lauren.html (12032) 100\n",
      "3107 http://python-data.dr-chuck.net/known_by_Maias.html (12073) 100\n",
      "1295 http://python-data.dr-chuck.net/known_by_Seumas.html (12076) 100\n",
      "2838 http://python-data.dr-chuck.net/known_by_Kofi.html (12038) 100\n",
      "4561 http://python-data.dr-chuck.net/known_by_Cliodhna.html (12047) 100\n",
      "694 http://python-data.dr-chuck.net/known_by_Iiona.html (12053) 100\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    if ( num < 1 ) :\n",
    "        sval = raw_input('How many pages:')\n",
    "        if ( len(sval) < 1 ) : break\n",
    "        num = int(sval)\n",
    "    num -= 1\n",
    "\n",
    "    cur.execute('SELECT id,url FROM Pages WHERE html is NULL and error is NULL ORDER BY RANDOM() LIMIT 1')\n",
    "    try:\n",
    "        row = cur.fetchone()\n",
    "        fromid = row[0]\n",
    "        url = row[1]\n",
    "    except:\n",
    "        print 'No unretrieved HTML pages found'\n",
    "        many = 0\n",
    "        break\n",
    "\n",
    "    print fromid, url, \n",
    "\n",
    "    # If we are retrieving this page, there should be no links from it\n",
    "    cur.execute('DELETE from Links WHERE from_id=?', (fromid, ) )\n",
    "    try:\n",
    "        document = urllib.urlopen(url)\n",
    "\n",
    "        html = document.read()\n",
    "        if document.getcode() != 200 :\n",
    "            print \"Error on page: \",document.getcode()\n",
    "            cur.execute('UPDATE Pages SET error=? WHERE url=?', (document.getcode(), url) )\n",
    "\n",
    "        if 'text/html' != document.info().gettype() :\n",
    "            print \"Ignore non text/html page\"\n",
    "            cur.execute('UPDATE Pages SET error=-1 WHERE url=?', (url, ) )\n",
    "            conn.commit()\n",
    "            continue\n",
    "\n",
    "        print '('+str(len(html))+')',\n",
    "\n",
    "        soup = BeautifulSoup(html)\n",
    "    except KeyboardInterrupt:\n",
    "        print ''\n",
    "        print 'Program interrupted by user...'\n",
    "        break\n",
    "    except:\n",
    "        print \"Unable to retrieve or parse page\"\n",
    "        cur.execute('UPDATE Pages SET error=-1 WHERE url=?', (url, ) )\n",
    "        conn.commit()\n",
    "        continue\n",
    "\n",
    "    cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( url, ) ) \n",
    "    cur.execute('UPDATE Pages SET html=? WHERE url=?', (buffer(html), url ) )\n",
    "    conn.commit()\n",
    "\n",
    "    # Retrieve all of the anchor tags\n",
    "    tags = soup('a')\n",
    "    count = 0\n",
    "    for tag in tags:\n",
    "        href = tag.get('href', None)\n",
    "        if ( href is None ) : continue\n",
    "        # Resolve relative references like href=\"/contact\"\n",
    "        up = urlparse(href)\n",
    "        if ( len(up.scheme) < 1 ) :\n",
    "            href = urljoin(url, href)\n",
    "        ipos = href.find('#')\n",
    "        if ( ipos > 1 ) : href = href[:ipos]\n",
    "        if ( href.endswith('.png') or href.endswith('.jpg') or href.endswith('.gif') ) : continue\n",
    "        if ( href.endswith('/') ) : href = href[:-1]\n",
    "        if ( len(href) < 1 ) : continue\n",
    "\n",
    "        found = False\n",
    "        for web in webs:\n",
    "            if ( href.startswith(web) ) :\n",
    "                found = True\n",
    "                break\n",
    "        if not found : continue\n",
    "\n",
    "        cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( href, ) ) \n",
    "        count = count + 1\n",
    "        conn.commit()\n",
    "\n",
    "        cur.execute('SELECT id FROM Pages WHERE url=? LIMIT 1', ( href, ))\n",
    "        try:\n",
    "            row = cur.fetchone()\n",
    "            toid = row[0]\n",
    "        except:\n",
    "            print 'Could not retrieve id'\n",
    "            continue\n",
    "        cur.execute('INSERT OR IGNORE INTO Links (from_id, to_id) VALUES ( ?, ? )', ( fromid, toid ) ) \n",
    "\n",
    "\n",
    "    print count\n",
    "\n",
    "cur.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
