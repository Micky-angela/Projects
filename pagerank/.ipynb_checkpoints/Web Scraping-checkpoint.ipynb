{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import urllib\n",
    "import ssl \n",
    "from urlparse import urljoin\n",
    "from urlparse import urlparse\n",
    "from BeautifulSoup import *\n",
    "scontext = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('data.sqlite')\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlite3.Cursor at 0x1128bfdc0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.execute('''CREATE TABLE IF NOT EXISTS Pages \n",
    "    (id INTEGER PRIMARY KEY, url TEXT UNIQUE, html TEXT, \n",
    "     error INTEGER, old_rank REAL, new_rank REAL)''')\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Links \n",
    "    (from_id INTEGER, to_id INTEGER)''')\n",
    "\n",
    "cur.execute('''CREATE TABLE IF NOT EXISTS Webs (url TEXT UNIQUE)''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter web url or enter: \n"
     ]
    }
   ],
   "source": [
    "cur.execute('SELECT id,url FROM Pages WHERE html is NULL and error is NULL ORDER BY RANDOM() LIMIT 1')\n",
    "row = cur.fetchone()\n",
    "if row is not None:\n",
    "    print \"Restarting existing crawl.\"\n",
    "else :\n",
    "    starturl = raw_input('Enter web url or enter: ')\n",
    "    if ( len(starturl) < 1 ) : starturl = 'http://python-data.dr-chuck.net/'\n",
    "    if ( starturl.endswith('/') ) : starturl = starturl[:-1]\n",
    "    web = starturl\n",
    "    if ( starturl.endswith('.htm') or starturl.endswith('.html') ) :\n",
    "        pos = starturl.rfind('/')\n",
    "        web = starturl[:pos]\n",
    "\n",
    "    if ( len(web) > 1 ) :\n",
    "        cur.execute('INSERT OR IGNORE INTO Webs (url) VALUES ( ? )', ( web, ) )\n",
    "        cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( starturl, ) ) \n",
    "        conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://python-data.dr-chuck.net']\n"
     ]
    }
   ],
   "source": [
    "cur.execute('''SELECT url FROM Webs''')\n",
    "webs = list()\n",
    "for row in cur:\n",
    "    webs.append(str(row[0]))\n",
    "\n",
    "print webs\n",
    "\n",
    "num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many pages:20\n",
      "1 http://python-data.dr-chuck.net (1405) 7\n",
      "6 http://python-data.dr-chuck.net/comments_42.json Ignore non text/html page\n",
      "3 http://python-data.dr-chuck.net/regex_sum_42.txt Ignore non text/html page\n",
      "7 http://python-data.dr-chuck.net/known_by_42.html (12018) 100\n",
      "106 http://python-data.dr-chuck.net/known_by_Finnlay.html (12072) 100\n",
      "201 http://python-data.dr-chuck.net/known_by_Galilee.html (12043) 100\n",
      "207 http://python-data.dr-chuck.net/known_by_Shae.html (12054) 100\n",
      "226 http://python-data.dr-chuck.net/known_by_Mahasen.html (12026) 100\n",
      "162 http://python-data.dr-chuck.net/known_by_Asrar.html (12043) 100\n",
      "204 http://python-data.dr-chuck.net/known_by_Raja.html (12053) 100\n",
      "575 http://python-data.dr-chuck.net/known_by_Ciar.html (12056) 100\n",
      "599 http://python-data.dr-chuck.net/known_by_Florence.html (12062) 100\n",
      "397 http://python-data.dr-chuck.net/known_by_Arun.html (12038) 100\n",
      "831 http://python-data.dr-chuck.net/known_by_Ericlee.html (12039) 100\n",
      "418 http://python-data.dr-chuck.net/known_by_Dre.html (12066) 100\n",
      "943 http://python-data.dr-chuck.net/known_by_Abbas.html (12080) 100\n",
      "760 http://python-data.dr-chuck.net/known_by_Ceiron.html (12056) 100\n",
      "533 http://python-data.dr-chuck.net/known_by_Ryley.html (12040) 100\n",
      "1235 http://python-data.dr-chuck.net/known_by_Ember.html (12034) 100\n",
      "453 http://python-data.dr-chuck.net/known_by_Gillian.html (12023) 100\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    if ( num < 1 ) :\n",
    "        sval = raw_input('How many pages:')\n",
    "        if ( len(sval) < 1 ) : break\n",
    "        num = int(sval)\n",
    "    num -= 1\n",
    "\n",
    "    cur.execute('SELECT id,url FROM Pages WHERE html is NULL and error is NULL ORDER BY RANDOM() LIMIT 1')\n",
    "    try:\n",
    "        row = cur.fetchone()\n",
    "        fromid = row[0]\n",
    "        url = row[1]\n",
    "    except:\n",
    "        print 'No unretrieved HTML pages found'\n",
    "        many = 0\n",
    "        break\n",
    "\n",
    "    print fromid, url, \n",
    "\n",
    "    # If we are retrieving this page, there should be no links from it\n",
    "    cur.execute('DELETE from Links WHERE from_id=?', (fromid, ) )\n",
    "    try:\n",
    "        document = urllib.urlopen(url)\n",
    "\n",
    "        html = document.read()\n",
    "        if document.getcode() != 200 :\n",
    "            print \"Error on page: \",document.getcode()\n",
    "            cur.execute('UPDATE Pages SET error=? WHERE url=?', (document.getcode(), url) )\n",
    "\n",
    "        if 'text/html' != document.info().gettype() :\n",
    "            print \"Ignore non text/html page\"\n",
    "            cur.execute('UPDATE Pages SET error=-1 WHERE url=?', (url, ) )\n",
    "            conn.commit()\n",
    "            continue\n",
    "\n",
    "        print '('+str(len(html))+')',\n",
    "\n",
    "        soup = BeautifulSoup(html)\n",
    "    except KeyboardInterrupt:\n",
    "        print ''\n",
    "        print 'Program interrupted by user...'\n",
    "        break\n",
    "    except:\n",
    "        print \"Unable to retrieve or parse page\"\n",
    "        cur.execute('UPDATE Pages SET error=-1 WHERE url=?', (url, ) )\n",
    "        conn.commit()\n",
    "        continue\n",
    "\n",
    "    cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( url, ) ) \n",
    "    cur.execute('UPDATE Pages SET html=? WHERE url=?', (buffer(html), url ) )\n",
    "    conn.commit()\n",
    "\n",
    "    # Retrieve all of the anchor tags\n",
    "    tags = soup('a')\n",
    "    count = 0\n",
    "    for tag in tags:\n",
    "        href = tag.get('href', None)\n",
    "        if ( href is None ) : continue\n",
    "        # Resolve relative references like href=\"/contact\"\n",
    "        up = urlparse(href)\n",
    "        if ( len(up.scheme) < 1 ) :\n",
    "            href = urljoin(url, href)\n",
    "        ipos = href.find('#')\n",
    "        if ( ipos > 1 ) : href = href[:ipos]\n",
    "        if ( href.endswith('.png') or href.endswith('.jpg') or href.endswith('.gif') ) : continue\n",
    "        if ( href.endswith('/') ) : href = href[:-1]\n",
    "        if ( len(href) < 1 ) : continue\n",
    "\n",
    "        found = False\n",
    "        for web in webs:\n",
    "            if ( href.startswith(web) ) :\n",
    "                found = True\n",
    "                break\n",
    "        if not found : continue\n",
    "\n",
    "        cur.execute('INSERT OR IGNORE INTO Pages (url, html, new_rank) VALUES ( ?, NULL, 1.0 )', ( href, ) ) \n",
    "        count = count + 1\n",
    "        conn.commit()\n",
    "\n",
    "        cur.execute('SELECT id FROM Pages WHERE url=? LIMIT 1', ( href, ))\n",
    "        try:\n",
    "            row = cur.fetchone()\n",
    "            toid = row[0]\n",
    "        except:\n",
    "            print 'Could not retrieve id'\n",
    "            continue\n",
    "        cur.execute('INSERT OR IGNORE INTO Links (from_id, to_id) VALUES ( ?, ? )', ( fromid, toid ) ) \n",
    "\n",
    "\n",
    "    print count\n",
    "\n",
    "cur.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
