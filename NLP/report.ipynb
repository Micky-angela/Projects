{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.svm import LinearSVC\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import random\n",
    "from sklearn.externals import joblib\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,BaggingClassifier,GradientBoostingClassifier\n",
    "import re\n",
    "import time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn import decomposition\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "\n",
    "Read the data from 6 csv file, each corresponds to data of one place.\n",
    "\n",
    "ny: New York mi: Miami bf: Buffalo fw: Fort Worth ys: Yellow Stone National Park gc: Grand Canyon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ny = pd.read_csv('ny_new.csv')\n",
    "mi = pd.read_csv('mi_new.csv')\n",
    "bf = pd.read_csv('bf_new.csv')\n",
    "fw = pd.read_csv('fw_new.csv')\n",
    "ys = pd.read_csv('ys_new.csv')\n",
    "gc = pd.read_csv('gc_new.csv')\n",
    "\n",
    "ny = shuffle(ny)\n",
    "mi = shuffle(mi)\n",
    "bf = shuffle(bf)\n",
    "fw = shuffle(fw)\n",
    "ys = shuffle(ys)\n",
    "gc = shuffle(gc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "Divide the whole dataset into train set and test set, and define all the functions needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = [ny[:17000],mi[:17000],bf[:6000],fw[:13000],ys[:8000],gc[:8000]]\n",
    "sample = pd.concat(df)\n",
    "df2 = [ny[17000:],mi[17000:],bf[6000:],fw[13000:],ys[8000:],gc[8000:]]\n",
    "test_text = pd.concat(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_text = list(sample['text'])\n",
    "rate = list(sample['overall_rate'])\n",
    "test_input = list(test_text['text'])\n",
    "test_rate = list(test_text['overall_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent = []\n",
    "for review in input_text:\n",
    "    sent.append(len(sent_tokenize(review)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent2 = []\n",
    "for review in test_input:\n",
    "    sent2.append(len(sent_tokenize(review)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(text): #define function to clean the data set\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    theText = text.split()\n",
    "    tokens = [re.sub(r'[^a-zA-Z!]+', ' ',word) for word in theText]\n",
    "    words = [wordnet_lemmatizer.lemmatize(token) for token in tokens]\n",
    "    words = [word.lower() for word in words if not word.isupper()] + [word for word in words if word.isupper()]\n",
    "    tokens = \" \".join(words)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean2(text): #define function to clean the data set\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    theText = text.split()\n",
    "    tokens = [re.sub(r'[^a-zA-Z!]+', ' ',word) for word in theText]\n",
    "    words = [wordnet_lemmatizer.lemmatize(token) for token in tokens]\n",
    "    words = [word.lower() for word in words if not word.isupper()] + [word for word in words if word.isupper()]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define function to call the model\n",
    "thePathLut = '/Users/AngelaChi/Documents/2017-9/projects files/'\n",
    "theLUT = pd.read_csv(thePathLut + 'classifierLUT.csv',index_col=0)\n",
    "def algoArray(theAlgo):     \n",
    "    theAlgoOut = theLUT.loc[theAlgo,'functionCall']\n",
    "    return theAlgoOut\n",
    "def optFunc(theAlgo,theParams):\n",
    "    theModel = theLUT.loc[theAlgo,'optimizedCall']\n",
    "    tempParam = list()\n",
    "    for key, value in theParams.iteritems(): \n",
    "        tempParam.append(str(key) + \"=\" + str(value)) \n",
    "    theParams = \",\".join(tempParam)\n",
    "    theModel = theModel + theParams + \")\"\n",
    "    return theModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define funtion to select certain kinds of words as features\n",
    "def pos_select(input_list):\n",
    "    total_pos = []\n",
    "    for i in input_list:\n",
    "        total_pos.append(nltk.pos_tag(i,tagset='universal'))\n",
    "    total_words = []\n",
    "    for words in total_pos:\n",
    "        lis = []\n",
    "        for i in words:\n",
    "            if str(i[1]) == 'ADJ' or str(i[1]) == 'ADV':\n",
    "                if len(str(i[0])) > 1:\n",
    "                    lis.append(str(i[0]))\n",
    "        total_words.append(lis)\n",
    "    words = []\n",
    "    for word in total_words:\n",
    "        word2 = \" \".join(word)\n",
    "        words.append(word2)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define funtion to select certain features\n",
    "def gram_select(total_terms):\n",
    "    one_gram = []\n",
    "    two_gram = []\n",
    "    three_gram = []\n",
    "    for term in total_terms:\n",
    "        if len(term[0].split()) == 1 and ((str(term[1]) == 'ADJ' or str(term[1]) == 'ADV') or str(term[0]).isupper()):\n",
    "            one_gram.append(term)\n",
    "        if len(term[0].split()) == 2 and (str(term[0].split()[0]) == 'no' or str(term[0].split()[0]) == 'not'):\n",
    "            two_gram.append(term)\n",
    "        if len(term[0].split()) == 3 and (str(term[0].split()[0]) == 'no' or str(term[0].split()[0]) == 'not'):\n",
    "            three_gram.append(term)\n",
    "    useful_word = one_gram + two_gram + three_gram\n",
    "    useful_word = [str(a) for a in list(zip(*useful_word))[0]]\n",
    "    return useful_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define funtion to train the model\n",
    "def train(res, rate):\n",
    "    theModels = ['RF','ABDT','BAG','GBC']\n",
    "    theResults = pd.DataFrame(0,index=theModels,columns=['accuracy','confidence','runtime'])\n",
    "    for theModel in theModels:\n",
    "        startTime = time.time()\n",
    "        model = eval(algoArray(theModel))\n",
    "        print(model)\n",
    "        #cross validation    \n",
    "        cvPerf = cross_val_score(model,res,rate,cv=10) \n",
    "        theResults.loc[theModel,'accuracy'] = round(cvPerf.mean(),2)\n",
    "        theResults.loc[theModel,'confidence'] = round(cvPerf.std() * 2,2)\n",
    "        endTime = time.time()\n",
    "        theResults.loc[theModel,'runtime'] = round(endTime - startTime,0)\n",
    "    return theResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define function to perform grid-search on parameters\n",
    "def grid_search(theResults, res, rate):\n",
    "    modelChoice = theResults['accuracy'].idxmax()              \n",
    "    startTime = time.time()\n",
    "    model = eval(algoArray(modelChoice))\n",
    "    grid = GridSearchCV(estimator=model, param_grid={\"n_estimators\": [10,30,50,100]})#eval(gridSearch(modelChoice))\n",
    "    grid.fit(res,rate)\n",
    "    bestScore = round(grid.best_score_,4)\n",
    "    parameters = grid.best_params_\n",
    "    endTime = time.time()\n",
    "    print(\"Best Score: \" + str(bestScore) + \" and Grid Search Time: \" + str(round(endTime - startTime,0)))\n",
    "    return modelChoice, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define function to perform feature selection\n",
    "def feature_selection(input_list, rate):\n",
    "    vectorizer = CountVectorizer(min_df = 40, max_df = 0.8, ngram_range = (1,3))\n",
    "    dtm = pd.DataFrame(vectorizer.fit_transform(input_list).toarray())\n",
    "    names = vectorizer.get_feature_names()\n",
    "    lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(dtm, rate)\n",
    "    model_s = SelectFromModel(lsvc, prefit=True)\n",
    "    X_new = pd.DataFrame(model_s.transform(dtm))\n",
    "    lis = list(model_s.get_support())\n",
    "    features = []\n",
    "    for i in range(len(lis)):\n",
    "        if lis[i] == True:\n",
    "            features.append(names[i])\n",
    "    X_new.columns = features\n",
    "    return X_new, vectorizer, lsvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define function to read in emotion dictionary\n",
    "def get_nrc_data():\n",
    "    nrc = \"NRC-emotion.txt\"\n",
    "    count=0\n",
    "    emotion_dict=dict()\n",
    "    with open(nrc,'r') as f:\n",
    "        all_lines = list()\n",
    "        for line in f:\n",
    "            if count < 46:\n",
    "                count+=1\n",
    "                continue\n",
    "            line = line.strip().split('\\t')\n",
    "            if int(line[2]) == 1:\n",
    "                if emotion_dict.get(line[0]):\n",
    "                    emotion_dict[line[0]].append(line[1])\n",
    "                else:\n",
    "                    emotion_dict[line[0]] = [line[1]]\n",
    "    return emotion_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define function to count numbers of words of different categories in review\n",
    "def emotion_analyzer(text):\n",
    "    #Set up the result dictionary\n",
    "    emotions = {x for y in emotion_dict.values() for x in y}\n",
    "    emotion_count = dict()\n",
    "    for emotion in emotions:\n",
    "        emotion_count[emotion] = 0\n",
    "    for word in text.split():\n",
    "        if emotion_dict.get(word):\n",
    "            for emotion in emotion_dict.get(word):\n",
    "                emotion_count[emotion] += 1.0\n",
    "    return emotion_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define function to choose model\n",
    "def model_choice(result,reducedTDM,rate):\n",
    "    para = 0\n",
    "    modelChoice = result['accuracy'].idxmax()\n",
    "    if modelChoice == 'RF':\n",
    "        model, para = grid_search(result, reducedTDM, rate)\n",
    "    else:\n",
    "        model = modelChoice\n",
    "    return model, para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_list = []\n",
    "for i in input_text:\n",
    "    input_list.append(clean(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_list = []\n",
    "for i in test_input:\n",
    "    test_list.append(clean(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1\n",
    "\n",
    "Use all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer1 = TfidfVectorizer(max_features=1000,ngram_range=(1,3))\n",
    "tdm1 = pd.DataFrame(vectorizer1.fit_transform(input_list).toarray())\n",
    "tdm1.columns=vectorizer1.get_feature_names()\n",
    "pca1 = decomposition.PCA(n_components=.95)\n",
    "pca1.fit(tdm1)\n",
    "reducedTDM1 = pd.DataFrame(pca1.transform(tdm1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=50,\n",
      "            verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "result1 = train(reducedTDM1, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.5133 and Grid Search Time: 1325.0\n",
      "Model Save Time: 1611.0\n"
     ]
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "model, para = model_choice(result1,reducedTDM1,rate)\n",
    "if para == 0:\n",
    "    model = eval(algoArray(model))\n",
    "else:\n",
    "    model = eval(optFunc(model,para))\n",
    "model.fit(reducedTDM1,rate)\n",
    "joblib.dump(model, 'basic_model.pkl') #save model\n",
    "endTime = time.time()\n",
    "print(\"Model Save Time: \" + str(round(endTime - startTime,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test1 = vectorizer1.transform(test_list)\n",
    "X2_new1 = pca1.transform(test1.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50358518518518514"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_rate, model.predict(X2_new1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2\n",
    "\n",
    "Only use adjective and adverbs in the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_list2 = []\n",
    "for i in input_text:\n",
    "    input_list2.append(list(clean2(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = pos_select(input_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer2 = CountVectorizer(min_df = 40, max_df = 0.85, ngram_range = (1,1))\n",
    "dtm2 = pd.DataFrame(vectorizer2.fit_transform(words).toarray())\n",
    "dtm2.columns=vectorizer2.get_feature_names()\n",
    "pca2 = decomposition.PCA(n_components=.95)\n",
    "pca2.fit(dtm2)\n",
    "reducedTDM2 = pd.DataFrame(pca2.transform(dtm2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=50,\n",
      "            verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "result2 = train(reducedTDM2, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.5202 and Grid Search Time: 1334.0\n",
      "Model Save Time: 1631.0\n"
     ]
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "model, para  = model_choice(result2,reducedTDM2,rate)\n",
    "if para == 0:\n",
    "    model = eval(algoArray(model))\n",
    "else:\n",
    "    model = eval(optFunc(model,para))\n",
    "model.fit(reducedTDM2,rate)\n",
    "joblib.dump(model, 'adj_model.pkl') #save model\n",
    "endTime = time.time()\n",
    "print(\"Model Save Time: \" + str(round(endTime - startTime,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_list2 = []\n",
    "for i in test_input:\n",
    "    test_list2.append(clean2(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_input = pos_select(test_list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50761481481481485"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test2 = vectorizer2.transform(test_input)\n",
    "X2_new2 = pca2.transform(test2.toarray())\n",
    "accuracy_score(test_rate, model.predict(X2_new2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3\n",
    "\n",
    "Use features with one to three word, and then use SVC to select features, and eliminate some useless features by hand.\n",
    "\n",
    "For features with only one word, only keep those that are adjectives or adverbs.\n",
    "\n",
    "For features with two or three word, only keep those with 'not' or 'no'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_new, vectorizer3, lsvc1 = feature_selection(input_list, rate)\n",
    "features = X_new.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_terms1 = nltk.pos_tag(features,tagset='universal')\n",
    "useful_word = gram_select(total_terms1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "308"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(useful_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_feature1 = X_new[useful_word]\n",
    "df_feature1 = df_feature1.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['able',\n",
       " 'absolutely',\n",
       " 'ac',\n",
       " 'actually',\n",
       " 'affordable',\n",
       " 'again',\n",
       " 'almost',\n",
       " 'already',\n",
       " 'also',\n",
       " 'always',\n",
       " 'amaury',\n",
       " 'amazing',\n",
       " 'anniversary',\n",
       " 'anywhere',\n",
       " 'atmosphere',\n",
       " 'attentive',\n",
       " 'available',\n",
       " 'average',\n",
       " 'away',\n",
       " 'awesome',\n",
       " 'awful',\n",
       " 'back',\n",
       " 'bad',\n",
       " 'barely',\n",
       " 'bartender',\n",
       " 'basic',\n",
       " 'basically',\n",
       " 'bath',\n",
       " 'beautiful',\n",
       " 'beautifully',\n",
       " 'bell',\n",
       " 'best',\n",
       " 'better',\n",
       " 'big',\n",
       " 'bigger',\n",
       " 'biggest',\n",
       " 'birthday',\n",
       " 'bottle',\n",
       " 'brian',\n",
       " 'bright',\n",
       " 'broadway',\n",
       " 'broken',\n",
       " 'busy',\n",
       " 'center',\n",
       " 'central',\n",
       " 'cheap',\n",
       " 'choose',\n",
       " 'classy',\n",
       " 'clean',\n",
       " 'clearly',\n",
       " 'close',\n",
       " 'closed',\n",
       " 'comfortable',\n",
       " 'complimentary',\n",
       " 'conveniently',\n",
       " 'convention',\n",
       " 'courteous',\n",
       " 'decent',\n",
       " 'definitely',\n",
       " 'delicious',\n",
       " 'different',\n",
       " 'difficult',\n",
       " 'disappointing',\n",
       " 'doe',\n",
       " 'double',\n",
       " 'downside',\n",
       " 'downtown',\n",
       " 'due',\n",
       " 'early',\n",
       " 'easy',\n",
       " 'efficient',\n",
       " 'elegant',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'enough',\n",
       " 'entire',\n",
       " 'especially',\n",
       " 'etc',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'everywhere',\n",
       " 'exactly',\n",
       " 'excellent',\n",
       " 'exceptional',\n",
       " 'exceptionally',\n",
       " 'expensive',\n",
       " 'extra',\n",
       " 'extremely',\n",
       " 'fabulous',\n",
       " 'fairly',\n",
       " 'fairway',\n",
       " 'faithful',\n",
       " 'fancy',\n",
       " 'fantastic',\n",
       " 'far',\n",
       " 'fast',\n",
       " 'favorite',\n",
       " 'few',\n",
       " 'filthy',\n",
       " 'finally',\n",
       " 'fine',\n",
       " 'first',\n",
       " 'fix',\n",
       " 'forward',\n",
       " 'free',\n",
       " 'fresh',\n",
       " 'friendly',\n",
       " 'full',\n",
       " 'good',\n",
       " 'gorgeous',\n",
       " 'grand',\n",
       " 'great',\n",
       " 'guess',\n",
       " 'guest',\n",
       " 'hallway',\n",
       " 'happy',\n",
       " 'hard',\n",
       " 'hear',\n",
       " 'here',\n",
       " 'hesitate',\n",
       " 'high',\n",
       " 'highly',\n",
       " 'historic',\n",
       " 'horrible',\n",
       " 'hospital',\n",
       " 'hot',\n",
       " 'how',\n",
       " 'however',\n",
       " 'huge',\n",
       " 'impeccable',\n",
       " 'impressed',\n",
       " 'incredible',\n",
       " 'inside',\n",
       " 'isn',\n",
       " 'just',\n",
       " 'kelly',\n",
       " 'kenier',\n",
       " 'key',\n",
       " 'kitchen',\n",
       " 'large',\n",
       " 'last',\n",
       " 'late',\n",
       " 'later',\n",
       " 'least',\n",
       " 'left',\n",
       " 'likely',\n",
       " 'limited',\n",
       " 'little',\n",
       " 'local',\n",
       " 'long',\n",
       " 'loud',\n",
       " 'lovely',\n",
       " 'low',\n",
       " 'main',\n",
       " 'many',\n",
       " 'mediocre',\n",
       " 'mile',\n",
       " 'minor',\n",
       " 'modern',\n",
       " 'mold',\n",
       " 'mondrian',\n",
       " 'more',\n",
       " 'most',\n",
       " 'much',\n",
       " 'negative',\n",
       " 'never',\n",
       " 'new',\n",
       " 'next',\n",
       " 'nice',\n",
       " 'nicely',\n",
       " 'nicest',\n",
       " 'not',\n",
       " 'now',\n",
       " 'old',\n",
       " 'older',\n",
       " 'only',\n",
       " 'other',\n",
       " 'otherwise',\n",
       " 'outdated',\n",
       " 'outstanding',\n",
       " 'overall',\n",
       " 'paint',\n",
       " 'patty',\n",
       " 'perfectly',\n",
       " 'personal',\n",
       " 'pet',\n",
       " 'pizza',\n",
       " 'pleasantly',\n",
       " 'pleased',\n",
       " 'pm',\n",
       " 'poor',\n",
       " 'poorly',\n",
       " 'positive',\n",
       " 'pretty',\n",
       " 'previous',\n",
       " 'pricey',\n",
       " 'private',\n",
       " 'probably',\n",
       " 'professional',\n",
       " 'quaint',\n",
       " 'quick',\n",
       " 'quickly',\n",
       " 'quiet',\n",
       " 'quite',\n",
       " 'rather',\n",
       " 'ready',\n",
       " 'really',\n",
       " 'reasonable',\n",
       " 'reasonably',\n",
       " 'recently',\n",
       " 'red',\n",
       " 'relatively',\n",
       " 'right',\n",
       " 'rim',\n",
       " 'roomy',\n",
       " 'safe',\n",
       " 'same',\n",
       " 'satisfied',\n",
       " 'second',\n",
       " 'separate',\n",
       " 'several',\n",
       " 'shore',\n",
       " 'short',\n",
       " 'shower',\n",
       " 'shuttle',\n",
       " 'simply',\n",
       " 'single',\n",
       " 'situated',\n",
       " 'sleep',\n",
       " 'slow',\n",
       " 'small',\n",
       " 'smaller',\n",
       " 'so',\n",
       " 'soft',\n",
       " 'sometimes',\n",
       " 'somewhat',\n",
       " 'somewhere',\n",
       " 'soon',\n",
       " 'south',\n",
       " 'spacious',\n",
       " 'speak',\n",
       " 'special',\n",
       " 'st',\n",
       " 'stain',\n",
       " 'stained',\n",
       " 'standard',\n",
       " 'still',\n",
       " 'strange',\n",
       " 'stuff',\n",
       " 'such',\n",
       " 'suite',\n",
       " 'sure',\n",
       " 'surprised',\n",
       " 'terrible',\n",
       " 'terrific',\n",
       " 'thank',\n",
       " 'then',\n",
       " 'thin',\n",
       " 'tiny',\n",
       " 'too',\n",
       " 'top',\n",
       " 'tried',\n",
       " 'truly',\n",
       " 'tusayan',\n",
       " 'typical',\n",
       " 'uncomfortable',\n",
       " 'unfortunately',\n",
       " 'updating',\n",
       " 've',\n",
       " 'very',\n",
       " 'wa',\n",
       " 'warm',\n",
       " 'website',\n",
       " 'well',\n",
       " 'west',\n",
       " 'when',\n",
       " 'where',\n",
       " 'white',\n",
       " 'why',\n",
       " 'wish',\n",
       " 'wonderful',\n",
       " 'worse',\n",
       " 'worst',\n",
       " 'worth',\n",
       " 'wrong',\n",
       " 'xanterra',\n",
       " 'yes',\n",
       " 'young',\n",
       " 'no complaint',\n",
       " 'no problem',\n",
       " 'no room',\n",
       " 'not bad',\n",
       " 'not be',\n",
       " 'not disappointed',\n",
       " 'not great',\n",
       " 'not have',\n",
       " 'not much',\n",
       " 'not only',\n",
       " 'not recommend',\n",
       " 'not so',\n",
       " 'not stay',\n",
       " 'not sure',\n",
       " 'not to',\n",
       " 'not too',\n",
       " 'not very',\n",
       " 'not work',\n",
       " 'not worth',\n",
       " 'not the best']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "useful_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=50,\n",
      "            verbose=0, warm_start=False)\n",
      "AdaBoostClassifier(algorithm='SAMME',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'),\n",
      "          learning_rate=1, n_estimators=300, random_state=None)\n",
      "BaggingClassifier(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "         verbose=0, warm_start=False)\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "              n_estimators=100, presort='auto', random_state=None,\n",
      "              subsample=1.0, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "result3 = train(df_feature1, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Save Time: 261.0\n"
     ]
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "model, para = model_choice(result3,df_feature1,rate)\n",
    "if para == 0:\n",
    "    model = eval(algoArray(model))\n",
    "else:\n",
    "    model = eval(optFunc(model,para))\n",
    "model.fit(df_feature1, rate)\n",
    "joblib.dump(model, 'self_model.pkl') #save model\n",
    "endTime = time.time()\n",
    "print(\"Model Save Time: \" + str(round(endTime - startTime,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test3 = pd.DataFrame(vectorizer3.transform(test_list).toarray())\n",
    "test_names = vectorizer3.get_feature_names()\n",
    "test3.columns = test_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_t = SelectFromModel(lsvc1, prefit=True)\n",
    "X_new_test = pd.DataFrame(model_t.transform(test3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lis2 = list(model_t.get_support())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features2 = []\n",
    "for i in range(len(lis2)):\n",
    "    if lis2[i] == True:\n",
    "        features2.append(test_names[i])\n",
    "X_new_test.columns = features2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "308"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_terms_test = nltk.pos_tag(features2,tagset='universal')\n",
    "useful_word_test = gram_select(total_terms_test)\n",
    "len(useful_word_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57351111111111108"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_rate, model.predict(X_new_test[useful_word_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>confidence</th>\n",
       "      <th>runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.07</td>\n",
       "      <td>39.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABDT</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1006.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BAG</th>\n",
       "      <td>0.52</td>\n",
       "      <td>0.08</td>\n",
       "      <td>348.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GBC</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.06</td>\n",
       "      <td>3592.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      accuracy  confidence  runtime\n",
       "RF        0.54        0.07     39.0\n",
       "ABDT      0.55        0.05   1006.0\n",
       "BAG       0.52        0.08    348.0\n",
       "GBC       0.58        0.06   3592.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 4\n",
    "\n",
    "Based on model 3, add over-sampling and under-sampling to the model.\n",
    "\n",
    "For those with rating under-represented in the data, over-sample with replacement.\n",
    "\n",
    "For those with rating over-represented in the data, under-sample without replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dis = sample.groupby(['overall_rate']).size().reset_index(name='counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bar = sample.shape[0]\n",
    "bar = bar/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_list = list(dis[dis['counts'] < bar]['overall_rate'])\n",
    "max_list = list(dis[dis['counts'] >= bar]['overall_rate'])\n",
    "df_min = sample[sample.overall_rate.isin(min_list)]\n",
    "df_max = sample[sample.overall_rate.isin(max_list)]\n",
    "sample1 = df_max.groupby('overall_rate').apply(lambda s: s.sample(bar)) \n",
    "sample2 = df_min.groupby('overall_rate').apply(lambda s: s.sample(bar, replace = True))\n",
    "sample_b = sample1.append(sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_text_b = list(sample_b['text'])\n",
    "rate_b = list(sample_b['overall_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_list_b = []\n",
    "for i in input_text_b:\n",
    "    input_list_b.append(clean(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_n, vectorizer4, lsvc2 = feature_selection(input_list_b, rate_b)\n",
    "features_n = X_n.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_terms2 = nltk.pos_tag(features_n,tagset='universal')\n",
    "useful_word_b = gram_select(total_terms2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "446"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(useful_word_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_feature2 = X_n[useful_word_b]\n",
    "df_feature2 = df_feature2.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=50,\n",
      "            verbose=0, warm_start=False)\n",
      "AdaBoostClassifier(algorithm='SAMME',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'),\n",
      "          learning_rate=1, n_estimators=300, random_state=None)\n",
      "BaggingClassifier(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "         verbose=0, warm_start=False)\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "              n_estimators=100, presort='auto', random_state=None,\n",
      "              subsample=1.0, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "result4 = train(df_feature2, rate_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7772 and Grid Search Time: 231.0\n",
      "Model Save Time: 266.0\n"
     ]
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "model, para = model_choice(result4,df_feature2,rate_b)\n",
    "if para == 0:\n",
    "    model = eval(algoArray(model))\n",
    "else:\n",
    "    model = eval(optFunc(model,para))\n",
    "model.fit(df_feature2,rate_b)\n",
    "joblib.dump(model, 'resample.pkl') #save model\n",
    "endTime = time.time()\n",
    "print(\"Model Save Time: \" + str(round(endTime - startTime,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test4 = pd.DataFrame(vectorizer4.transform(test_list).toarray())\n",
    "test_names4 = vectorizer4.get_feature_names()\n",
    "test4.columns = test_names4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_t2 = SelectFromModel(lsvc2, prefit=True)\n",
    "X_new_test2 = pd.DataFrame(model_t2.transform(test4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50543"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lis3 = list(model_t2.get_support())\n",
    "len(lis3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features3 = []\n",
    "for i in range(len(lis3)):\n",
    "    if lis3[i] == True:\n",
    "        features3.append(test_names4[i])\n",
    "X_new_test2.columns = features3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "446"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_terms_test2 = nltk.pos_tag(features3,tagset='universal')\n",
    "useful_word_test2 = gram_select(total_terms_test2)\n",
    "len(useful_word_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5510518518518519"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_rate, model.predict(X_new_test2[useful_word_test2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 5\n",
    "\n",
    "Based on model 3, add the number of sentences of review as a new input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_feature1 = pd.DataFrame(df_feature1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_feature1['length'] = sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=50,\n",
      "            verbose=0, warm_start=False)\n",
      "AdaBoostClassifier(algorithm='SAMME',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'),\n",
      "          learning_rate=1, n_estimators=300, random_state=None)\n",
      "BaggingClassifier(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "         verbose=0, warm_start=False)\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "              n_estimators=100, presort='auto', random_state=None,\n",
      "              subsample=1.0, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "result5 = train(df_feature1, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Save Time: 205.0\n"
     ]
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "model, para = model_choice(result5,df_feature1,rate)\n",
    "if para == 0:\n",
    "    model = eval(algoArray(model))\n",
    "else:\n",
    "    model = eval(optFunc(model,para))\n",
    "model.fit(df_feature1,rate)\n",
    "joblib.dump(model, 'length.pkl') #save model\n",
    "endTime = time.time()\n",
    "print(\"Model Save Time: \" + str(round(endTime - startTime,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/AngelaChi/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X_new_test3 = X_new_test[useful_word_test]\n",
    "X_new_test3['length'] = sent2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57019259259259258"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_rate, model.predict(X_new_test3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accuracy</th>\n",
       "      <th>confidence</th>\n",
       "      <th>runtime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RF</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.07</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ABDT</th>\n",
       "      <td>0.55</td>\n",
       "      <td>0.06</td>\n",
       "      <td>364.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BAG</th>\n",
       "      <td>0.53</td>\n",
       "      <td>0.08</td>\n",
       "      <td>329.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GBC</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.05</td>\n",
       "      <td>1848.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      accuracy  confidence  runtime\n",
       "RF        0.54        0.07     32.0\n",
       "ABDT      0.55        0.06    364.0\n",
       "BAG       0.53        0.08    329.0\n",
       "GBC       0.58        0.05   1848.0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 6\n",
    "\n",
    "Use the sentiment polarity scores of review text as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "for review in input_text:\n",
    "    res.append(analyser.polarity_scores(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = pd.DataFrame.from_records(res)\n",
    "res = res.drop(['compound'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=50,\n",
      "            verbose=0, warm_start=False)\n",
      "AdaBoostClassifier(algorithm='SAMME',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'),\n",
      "          learning_rate=1, n_estimators=300, random_state=None)\n",
      "BaggingClassifier(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "         verbose=0, warm_start=False)\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "              n_estimators=100, presort='auto', random_state=None,\n",
      "              subsample=1.0, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "result6 = train(res, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Save Time: 5.0\n"
     ]
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "model, para = model_choice(result6,res,rate) #train fully validated and optimized model\n",
    "if para == 0:\n",
    "    model = eval(algoArray(model))\n",
    "else:\n",
    "    model = eval(optFunc(model,para))\n",
    "model.fit(res,rate)\n",
    "joblib.dump(model, 'rate.pkl') #save model\n",
    "endTime = time.time()\n",
    "print(\"Model Save Time: \" + str(round(endTime - startTime,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res2 = []\n",
    "for review in test_input:\n",
    "    res2.append(analyser.polarity_scores(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res2 = pd.DataFrame.from_records(res2)\n",
    "res2 = res2.drop(['compound'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52272592592592593"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_rate, model.predict(res2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 7\n",
    "\n",
    "Use the number of words in different sentiment categories of each review text as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emotion_dict = get_nrc_data()\n",
    "review_emotion_dic = {1:{},2:{},3:{},4:{},5:{}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = []\n",
    "for review in input_text:\n",
    "    res.append(emotion_analyzer(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = pd.DataFrame.from_records(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=50,\n",
      "            verbose=0, warm_start=False)\n",
      "AdaBoostClassifier(algorithm='SAMME',\n",
      "          base_estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best'),\n",
      "          learning_rate=1, n_estimators=300, random_state=None)\n",
      "BaggingClassifier(base_estimator=None, bootstrap=True,\n",
      "         bootstrap_features=False, max_features=1.0, max_samples=1.0,\n",
      "         n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "         verbose=0, warm_start=False)\n",
      "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "              min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "              n_estimators=100, presort='auto', random_state=None,\n",
      "              subsample=1.0, verbose=0, warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "result7 = train(res, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Save Time: 21.0\n"
     ]
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "model, para = model_choice(result7,res,rate) #train fully validated and optimized model\n",
    "if para == 0:\n",
    "    model = eval(algoArray(model))\n",
    "else:\n",
    "    model = eval(optFunc(model,para))\n",
    "model.fit(res,rate)\n",
    "joblib.dump(model, 'rate2.pkl') #save model\n",
    "endTime = time.time()\n",
    "print(\"Model Save Time: \" + str(round(endTime - startTime,0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res2 = []\n",
    "for review in test_input:\n",
    "    res2.append(emotion_analyzer(review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res2 = pd.DataFrame.from_records(res2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49315555555555557"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(test_rate, model.predict(res2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
